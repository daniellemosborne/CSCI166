{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Frozen Lake Domain Description\n",
        "\n",
        "Frozen Lake is a simple grid-world environment where an agent navigates a frozen lake to reach a goal while avoiding falling into holes. The environment is represented as a grid, with each cell being one of the following:\n",
        "\n",
        "* **S**: Starting position of the agent\n",
        "* **F**: Frozen surface, safe to walk on\n",
        "* **H**: Hole, falling into one ends the episode with a reward of 0\n",
        "* **G**: Goal, reaching it ends the episode with a reward of 1\n",
        "\n",
        "The agent can take four actions:\n",
        "\n",
        "* **0: Left**\n",
        "* **1: Down**\n",
        "* **2: Right**\n",
        "* **3: Up**\n",
        "\n",
        "However, due to the slippery nature of the ice, the agent might not always move in the intended direction. There's a chance it moves perpendicular to the intended direction.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hzTUHNC0Oien"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKf_jjk9OgN1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d527927-64d7-4045-8c44-94aae042a80c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Left)\n",
            "\u001b[41mS\u001b[0mFFF\n",
            "FHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:241: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "  if not isinstance(terminated, (bool, np.bool8)):\n"
          ]
        }
      ],
      "source": [
        "import gym\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make('FrozenLake-v1', render_mode='ansi')  # 'ansi' mode for text-based rendering\n",
        "\n",
        "# Reset the environment to the initial state\n",
        "observation = env.reset()\n",
        "\n",
        "# Take a few actions and observe the results\n",
        "for _ in range(5):\n",
        "    action = env.action_space.sample()  # Choose a random action\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    # Render the environment to see the agent's movement (text-based)\n",
        "    if done:\n",
        "        observation= env.reset()\n",
        "    else:\n",
        "      rendered = env.render()\n",
        "      if len(rendered) > 1:  # Check if there's a second element\n",
        "         print(rendered[1])  # Print the second element\n",
        "# Close the environment\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The transition model for the Frozen Lake world describes how the agent's actions affect its movement and the resulting state transitions. Here's a breakdown of the key components:\n",
        "\n",
        "**Actions:**\n",
        "\n",
        "* The agent can choose from four actions:\n",
        "    * 0: Left\n",
        "    * 1: Down\n",
        "    * 2: Right\n",
        "    * 3: Up\n",
        "\n",
        "**State Transitions:**\n",
        "\n",
        "* **Intended Movement:** Ideally, the agent moves one cell in the chosen direction.\n",
        "* **Slippery Ice:** Due to the slippery nature of the ice, there's a probability that the agent will move in a perpendicular direction instead of the intended one. The exact probabilities depend on the specific Frozen Lake environment configuration, but typically:\n",
        "    * **Successful Move:** The agent moves in the intended direction with a high probability.\n",
        "    * **Perpendicular Move:** The agent moves 90 degrees to the left or right of the intended direction with a lower probability.\n",
        "* **Boundaries:** If the intended movement would take the agent outside the grid boundaries, it remains in its current position.\n",
        "* **Holes:** If the agent lands on a hole (\"H\"), the episode ends, and it receives a reward of 0.\n",
        "* **Goal:** If the agent reaches the goal (\"G\"), the episode ends, and it receives a reward of 1.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R_q5-OvYOldL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "\n",
        "# Create the environment\n",
        "env = gym.make('FrozenLake-v1', render_mode='ansi')  # 'ansi' mode for text-based rendering\n",
        "\n",
        "# Reset the environment to the initial state\n",
        "observation = env.reset()\n",
        "\n",
        "# Take a few actions and observe the results\n",
        "for _ in range(5):\n",
        "    action = env.action_space.sample()  # Choose a random action\n",
        "    observation, reward, done, info = env.step(action)\n",
        "    # Render the environment to see the agent's movement (text-based)\n",
        "    if done:\n",
        "        observation= env.reset()\n",
        "    else:\n",
        "      rendered = env.render()\n",
        "      if len(rendered) > 1:  # Check if there's a second element\n",
        "         print(rendered[1])  # Print the second element\n",
        "# Close the environment\n",
        "env.close()\n",
        "print (\"State 14 Going Right: (s, a, r, Done)\", env.P[14][2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7nU_-9uaOQR",
        "outputId": "b4eaf737-b792-4082-8d8b-392fa223323d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (Right)\n",
            "SFFF\n",
            "\u001b[41mF\u001b[0mHFH\n",
            "FFFH\n",
            "HFFG\n",
            "\n",
            "State 14 Going Right: (s, a, r, Done) [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# Create FrozenLake environment\n",
        "env = gym.make(\"FrozenLake-v1\")\n",
        "\n",
        "# Starter code for students (modified for number of iterations)\n",
        "def value_iteration(env, gamma=0.9, num_iterations=1000):\n",
        "    \"\"\"\n",
        "    Implements the Value Iteration algorithm.\n",
        "\n",
        "    Args:\n",
        "        env: The OpenAI Gym environment.\n",
        "        gamma: Discount factor.\n",
        "        num_iterations: Number of iterations to run.\n",
        "\n",
        "    Returns:\n",
        "        The optimal value function and policy.\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize value function and policy\n",
        "    V = np.zeros(env.observation_space.n)\n",
        "    policy = np.zeros(env.observation_space.n)\n",
        "\n",
        "    # TODO: Implement the core Value Iteration logic here\n",
        "    # - Iterate for 'num_iterations'\n",
        "    for _ in range(num_iterations):\n",
        "      # copying values of V into a temp variable\n",
        "      V_temp = np.copy(V)\n",
        "      # for each state in our environment\n",
        "      for state in range(env.observation_space.n):\n",
        "        # intializing Q values with zeros\n",
        "        Q = np.zeros(env.action_space.n)\n",
        "        # for each action we take in our environment\n",
        "        for action in range(env.action_space.n):\n",
        "          for prob, next_state, reward, done in env.P[state][action]:\n",
        "            # calculating Q values for all actions\n",
        "            Q[action] += prob * (reward + gamma * V_temp[next_state])\n",
        "        # updating V[s] with the max Q value\n",
        "        V[state] = max(Q)\n",
        "        # updating policy with the action that maximizes Q value\n",
        "        policy[state] = np.argmax(Q)\n",
        "\n",
        "    # - For each state:\n",
        "    #   - Calculate Q values for all actions\n",
        "    #   - Update V[s] with the max Q value\n",
        "    #   - Update policy[s] with the action that maximizes Q value\n",
        "\n",
        "    # Assume that all initial states will be 0\n",
        "\n",
        "    return V, policy\n",
        "\n",
        "# Apply student's Value Iteration\n",
        "optimal_V, optimal_policy = value_iteration(env)\n",
        "\n",
        "# Evaluate student's solution (Optional)\n",
        "def evaluate_policy(env, policy, num_episodes=100):\n",
        "    total_reward = 0\n",
        "    for _ in range(num_episodes):\n",
        "        state = env.reset()\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = policy[state]\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            total_reward += reward\n",
        "    return total_reward / num_episodes\n",
        "\n",
        "average_reward = evaluate_policy(env, optimal_policy)\n",
        "print(\"Optimal Policy:\")\n",
        "print(optimal_policy.reshape(4, 4))\n",
        "print(\"Optimal Values:\")\n",
        "print(optimal_V.reshape(4, 4))\n",
        "print(\"Average Reward:\", average_reward)"
      ],
      "metadata": {
        "id": "U92a-f0HO1j7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ec6b4de1-c67a-4ab9-f31a-f60d1a893b4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal Policy:\n",
            "[[0. 3. 0. 3.]\n",
            " [0. 0. 0. 0.]\n",
            " [3. 1. 0. 0.]\n",
            " [0. 2. 1. 0.]]\n",
            "Optimal Values:\n",
            "[[0.0688909  0.06141457 0.07440976 0.05580732]\n",
            " [0.09185454 0.         0.11220821 0.        ]\n",
            " [0.14543635 0.24749695 0.29961759 0.        ]\n",
            " [0.         0.3799359  0.63902015 0.        ]]\n",
            "Average Reward: 0.72\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Results:\n",
        "\n",
        "The above results display the optimal policy, the optimal values, and the average reward.\n",
        "\n",
        "*   The optimal policy matrix represents the best action to take in each state, with 0 being Left, 1 being Down, 2 being Right, and 3 being Up.\n",
        "*   The optimal values matrix shows the expected cumulative reward you can achieve from each state.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "IPgmrryWlB-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "# Q Learning Algorithm\n",
        "\n",
        "# create a frozen lake environment\n",
        "env = gym.make(\"FrozenLake-v1\")\n",
        "n_observations = env.observation_space.n\n",
        "n_actions = env.action_space.n\n",
        "\n",
        "# creating and initializing the q-table to 0\n",
        "Q_table = np.zeros((n_observations, n_actions))\n",
        "\n",
        "# number of episodes we will run\n",
        "num_episodes = 10000\n",
        "# max iterations per episode\n",
        "max_itrs_per_episode = 100\n",
        "# initialize the exploration probability to 1\n",
        "exploration_prob = 1\n",
        "# exploration decreasing decay for exponential decreasing\n",
        "exploration_dec_decay = 0.001\n",
        "# minimum of exploration probability\n",
        "min_exploration_prob = 0.01\n",
        "# gamma discount\n",
        "gamma = 0.9\n",
        "# learning rate\n",
        "alpha = 0.1\n",
        "\n",
        "# storing the total rewards the agent gets in the environment after each episode in a list\n",
        "total_rewards_episode = []\n",
        "\n",
        "# iterating over episodes\n",
        "for e in range(num_episodes):\n",
        "    # initialize the first state of the episode\n",
        "    current_state = env.reset()\n",
        "    done = False\n",
        "\n",
        "    # sum the rewards that the agent gets from the environment\n",
        "    total_episode_reward = 0\n",
        "\n",
        "    for i in range(max_itrs_per_episode):\n",
        "        # exploration\n",
        "        if np.random.uniform(0, 1) < exploration_prob:\n",
        "            action = env.action_space.sample()\n",
        "        # exploitation\n",
        "        else:\n",
        "            action = np.argmax(Q_table[current_state, :])\n",
        "\n",
        "        # the environment runs the chosen action and returns the next state, reward, and done\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        # update our q-table using the q-learning iteration\n",
        "        Q_table[current_state, action] = (1 - alpha) * Q_table[current_state, action] + alpha * (reward + gamma * max(Q_table[next_state, :]))\n",
        "\n",
        "        # update total reward\n",
        "        total_episode_reward += reward\n",
        "\n",
        "        # if the episode is done, exit the loop\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "        # update current state\n",
        "        current_state = next_state\n",
        "\n",
        "    # append the total reward for the episode after it finishes\n",
        "    total_rewards_episode.append(total_episode_reward)\n",
        "\n",
        "    # update the exploration probability using exponential decay\n",
        "    exploration_prob = max(min_exploration_prob, np.exp(-exploration_dec_decay * e))\n",
        "\n",
        "# print average reward per thousand episodes\n",
        "print(\"Average reward per thousand episodes:\")\n",
        "for i in range(10):\n",
        "    print(\"(\", (i + 1) * 1000, \"): Average reward: \", np.mean(total_rewards_episode[1000 * i:1000 * (i + 1)]))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TfaR0s9pGC5k",
        "outputId": "2763b4cf-9b18-435c-bfce-bdbefc371383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average reward per thousand episodes:\n",
            "( 1000 ): Average reward:  0.04\n",
            "( 2000 ): Average reward:  0.173\n",
            "( 3000 ): Average reward:  0.323\n",
            "( 4000 ): Average reward:  0.336\n",
            "( 5000 ): Average reward:  0.524\n",
            "( 6000 ): Average reward:  0.515\n",
            "( 7000 ): Average reward:  0.497\n",
            "( 8000 ): Average reward:  0.425\n",
            "( 9000 ): Average reward:  0.494\n",
            "( 10000 ): Average reward:  0.406\n"
          ]
        }
      ]
    }
  ]
}